# ARC Model Endpoint Configuration
# ==================================
# Defines all available LLM models and their endpoints.
#
# Each model has:
# - id: Unique model identifier
# - name: Human-readable name
# - endpoint: API endpoint URL
# - provider: Provider type (vllm, anthropic, openai, local, mock)
# - context_window: Maximum context length in tokens
# - max_output_tokens: Maximum output tokens
# - offline: Whether model runs offline
# - api_key_required: Whether API key is needed (optional)

models:
  # === PRIMARY REASONING MODELS ===

  - id: deepseek-r1
    name: "DeepSeek R1"
    endpoint: "http://localhost:8000/generate"
    provider: vllm
    context_window: 32768
    max_output_tokens: 4096
    offline: false
    api_key_required: false

  - id: claude-sonnet-4.5
    name: "Claude Sonnet 4.5"
    endpoint: "https://api.anthropic.com/v1/messages"
    provider: anthropic
    context_window: 200000
    max_output_tokens: 8192
    offline: false
    api_key_required: true

  # === SPECIALIZED MODELS ===

  - id: qwen2.5-32b
    name: "Qwen 2.5 32B"
    endpoint: "http://localhost:8001/generate"
    provider: vllm
    context_window: 32768
    max_output_tokens: 4096
    offline: false
    api_key_required: false

  # === LOCAL/OFFLINE MODELS ===

  - id: llama-3-8b-local
    name: "Llama 3 8B (Local)"
    endpoint: "http://localhost:8002/generate"
    provider: local
    context_window: 8192
    max_output_tokens: 2048
    offline: true
    api_key_required: false

  # === MOCK MODEL (DEVELOPMENT) ===

  - id: mock-llm
    name: "Mock LLM (Offline)"
    endpoint: "mock://offline"
    provider: mock
    context_window: 100000
    max_output_tokens: 10000
    offline: true
    api_key_required: false

# === MODEL PROVIDER NOTES ===
#
# vllm: Self-hosted using vLLM server (http://localhost:8000)
# anthropic: Anthropic API (requires API key via env var ANTHROPIC_API_KEY)
# openai: OpenAI API (requires API key via env var OPENAI_API_KEY)
# local: Local model server (Ollama, llama.cpp, etc.)
# mock: Offline mock for development/testing (no network calls)
#
# === EXAMPLE ENVIRONMENT VARIABLES ===
#
# export ANTHROPIC_API_KEY="sk-ant-..."
# export OPENAI_API_KEY="sk-..."
# export ARC_DEFAULT_MODEL="deepseek-r1"
# export ARC_OFFLINE_MODE="true"  # Use only offline models
